# -*- coding: utf-8 -*-
"""Predicting House Price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NWKTWyj1wcKs86ifC2G_-Tmfa79HgY5c
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

"""# Problem Statement

With the dataset that describes the houses we have to predict the house price.

## Creating a DataFrame
"""

california = fetch_california_housing()
df= pd.DataFrame(california.data)

"""### EDA - Exploratory Data Analysis"""

df.head(10)

df.columns = california.feature_names

df.head()

"""# 8 numeric, predictive attributes and the target

**Attribute Information:**

MedInc median income in block group

HouseAge median house age in block group

AveRooms average number of rooms per household

AveBedrms average number of bedrooms per household

Population block group population

AveOccup average number of household members

Latitude block group latitude

Longitude block group longitude

#Creating a column 'Price'
**We need labled data ie the price of the houses to make the model learn from the labled data**
"""

df['PRICE']=california.target

"""**target is the lable ie an array of prices and california is the variable has the dataset**

"""

df.head()

df.tail()

df.shape

df.columns
df.dtypes

df.columns

df.isnull()

df.isnull().sum()

"""**All the above columns have 0 null values, so we can see that the dataset is cleaned & pre-processed**"""

df.describe()

df.corr

plt.figure(figsize=(10,10))
sns.heatmap(data=df.corr(), cmap='Greens')

"""-ve values give inversely related correlation and +ve values are directly related

# Median income directly and highly dependent on price
when 2 of the values are very closelt related eg (0.91) we can remove one of the columns (this process is known as feature reduction)

a text added to a diagram is called annotation.
"""

plt.figure(figsize=(10,10))
sns.heatmap(data=df.corr(), annot= True, cmap='Greens')

"""Darker the color, More corelation"""

sns.pairplot(df, size=5)

"""The above pair plot gives the skewness, data of every single column with other column"""

plt.figure(figsize=(50,50))
df.boxplot()

"""graph 4 has lot of outliers (points outside the range or box). Outlier is an observation that lies an abnormal distance from other values in a random sample from a population. Outliers drastically impact the machine learning model. Outliers are important in cyber security."""

df.PRICE.min()

df.PRICE.max()

df.PRICE.std()

"""End Of EDA"""

df.to_csv()

sns.pairplot(df, size=5)

df.to_csv('california_dataset.csv')

"""## Machine Learning - Linear Regression

**Separating the input and the output**
"""

X = np.array(df.drop('PRICE', axis=1))
y = np.array(df.PRICE)

"""# Splitting the data (training and testing)

20% of data as test data (size of test data) and we are randomly arranging the data. algorithm could learn with different sections of the data
"""

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

len(X_train)

len(y_train)

len(X_test)

len(y_test)

"""# Choosing the model"""

model = LinearRegression()

"""# Fitting/Train the model"""

model.fit(X_train,y_train)

model.intercept_

model.coef_

"""# Prediction"""

y_test

y_pred=model.predict(X_test)

y_pred

"""# Testing the model Performance"""

model.score(X_test,y_test)

r2_score(y_test,y_pred)

mean_squared_error(y_test,y_pred)

plt.scatter(y_test,y_pred)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.grid()
plt.plot([min(y_test),max(y_test)], [min(y_pred),max(y_pred)])
plt.title('Actual Price V/s Predicted Price')